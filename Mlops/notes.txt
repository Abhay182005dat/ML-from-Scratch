first always git init then dvc init or dvc init --subdir if inside any subfolders

A DVC stage is one step in your ML workflow that turns inputs into outputs using a command.
Real-life analogy ( for myself ) 
Making tea â˜•:
Step	                Stage
- Boil water	        Stage 1
- Add tea leaves	    Stage 2
- Add milk & sugar	    Stage 3

-n name -d src/data...(the folder) -p params(if any) -o data/raw(the output folder)
example : dvc stage add -n data_ingestion -d src/data_ingestion.py -o data/raw python src/data_ingestion.py

YAML - yaml aint a markup language , more readable commands for pipelines, used for writing config files in devops

** dvc repro -> runs the pipeline
** dvc dag -> visualise the pipeline
** specify previous directory or raw data as input to next pipeline ! -> dvc stage add -n data_preprocessing 
                    -d src/data_preprocessing.py -d data/raw -o data/processed python src/data_preprocessing.py

                                                                        explicitly tell dvc about metrics file
** dvc stage add -n model_evaluation -d src/model_evaluation.py -d model.pkl --metrics metrics.json  python src/model_evaluation.py 

** dvc metrics show  -> track your metrics 



// after you make a version normally do this
git add .
got commit -m '' 
git tag -a v1.0 -m 'release V1 with Bag of Words' 
git push 

what we did in vc-emotion-detection was that we changed the countvectorizer to tfidf to practice versioning

git checkout tags/v1.0 -> takes you to version 1 which consisted bag of words feature engineering step

```````````````````````````````     Steps for data versioning using DVC `````````````````````````````````````
Data versioning folder -
in this session you tracked only data without any pipelines , but when you use dvc pipelines such as 
data_ingestion, data_processing etc .. dvc automatically tracks your outputs of pipelines in "out" commmand in
dvc.yaml file , so whenever you use pipelines there no need to separately do dvc add etc, its tracked in dvc.lock file


Step 1: Explain the problem statement
Step 2: Create a project directory/ and data, src directories
Step 3: Initialize git
Step 4: Initialize DVC [Explanation of files]
Step 5: Create a GitHub repository
Step 6: Add remote for git                              echo %TEMP% -> show your cache directory for data versioning    
Step 7: Add remote for DVC [Changes in config file]     dvc remote add -d myremote C:\Users\abhay\AppData\Local\Temp\any_name
Step 8: Git add -> First commit [DVC config files]
Step 9: Write python code -> Run [Git status/DVC Status]
Step 10: DVC add [Multiple things happen here]              dvc add data/customer.csv

when you do step 10 the cache folder is made and a .gitignore file is made inside data so that data isn't pushed to github and
data is only pushed to dvc caches and a .dvc file is made to be pushed to github so that only metadata of cache file is pushed
to the github.

Step 11: Git add code/dvc files
Step 12: Git commit
Step 13: DVC push to remote
Step 14: Git push to GitHub [Experiment 1]
Step 15: Start experiment 2 -> Change code -> run
Step 16: Git status/ dvc status
Step 17: dvc add                              (no need to dvc add while using pipelines you can directly 
                                                dvc push and git push       )
Step 18: git add
Step 19: git commit
Step 20: dvc push -> git push

in simple way :
run you python file which generate dataset or anything like file '
git status
dvc add folder_pathof_data/---.csv
git add .
git commit 
dvc push
git push

now in the second version of data you just add some line in data_ingestion file 
df.drop(columns=['Avg. Session Length'],inplace = True)

git log --oneline to check all commits once

** to rollback first you do  "git checkout" then you do "dvc checkout" ,then dvc auto checkout using the .dvc file from git fetched files
(above method also applies while using pipelines also)

** also after cloning a repo you Start with **dvc fetch** and then **dvc checkout** also (dvc fetch downloads data to cache directory only)
** dvc pull downloads data from cache to your workspacefolder ( if you do dvc pull no need of checkout as it does both)


``````````````````if you use AWS  as your remote storage then do below steps `````````````````````````````

whenever you create a S3 bucket you can not directly connect to it , for that you create IAM user to connect 
it securely.IAM is where you can make different users and can give them different roles like access etc.

Create S3 bucket 
Create IAM User -> you make a policy etc
    go to users -> username -> attach policies -> create policy -> 

    {
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": ["s3:ListBucket"],
			"Resource": ["arn:aws:s3:::xxxxxxx"]  -> get your arn number from bucket's properties
		},
		{
		  	"Effect": "Allow",
			"Action": ["s3:PutObject", "s3:GetObject"],
			"Resource": ["arn:aws:s3:::xxxxxxxx/*"]  
		}
	]
}
-> after this give a name (allowdvc) ->after creating the user -> create access key -> cli ->create


pip install dvc[s3]
pip install awscli
git init
dvc init
git remote add
aws configure
dvc remote add -d dvcstore s3://name_of_bucket
dvc add
git commit 
dvc push 
git push

``````````````````````````````````` experiment tracking with DVC `````````````````````````````````````````
not done in this project folder because of some issues

install dvclive used for experiment tracking

** live.log_metric() in DVC logs numeric metrics during model training and integrates 
				  them with DVC experiments for versioning and comparison.
similarly log_param() , 

** dvc exp show -> shows all the experiments you have done in command
now when you change params and run your python file again and again dvc keeps track of your scores or metrics or experiments

workflow -> run python src/somefile.py -> dvc exp show 
also you can download dvc extension and manage from there

** dvc exp remove -?used for removing experiments
** dvc exp apply <exp-name> --> you go back to the workspace of that experiment or you can also apply a specific(best) experiment to your workspace.
** dvc exp diff <exp-name1> <exp-name2> --> you can see difference between two experiments
** dvc exp run --queue -S model_training.n_estimators = 8,16,32 -S model_training.max_depth=2,3,5 
	( model_training.n_estimators these are from params.yaml file you are tracking multiple experiments like gridsearchCV )

when to use 'dvc repro' ?? 
-> purpose : the dvc repro is used to reproduce your pipeline when any of the dependencies change. It ensures that all 
				stages of the pipeline are executed in the correct order based on the dependency graph defined in your dvc.yaml file

use cases : regular pipeline execution -> use dvc repro when you want to run your pipeline as defined , typically 
				after changing the code , input data , or configuration files(like params.yaml)

			reproducing results -> use it to reproduce results or ensure that your pipeline produces consistent outputs after 
									making changes.

when to use 'dvc exp run ' ?? 

dvc exp run

Purpose: The "dvc exp run"{} command is designed for running and managing experiments. It allows you to tweak 
		parameters, run different configurations, and keep track of multiple experimental results without altering 
		your main pipeline.

Use Cases :

1. Running Experiments: Use 'dvc exp run' when you want to experiment with different I hyperparameters or 
	configurations and compare the results.

2. Parameter Sweeps: Use it for hyperparameter tuning by specifying different parameter values directly in 
	the command or by queueing multiple experiments.

3. Experiment Tracking: Use it to track and compare experimental results without modifying the main branch or 
	affecting the primary workflow.



````````````````````````````````````````````````````MlFlow````````````````````````````````````````````````````````````````


Why choose mlflow over DVC ? 
-> In dvc you do through command line which is a little bit hassle and also the extension is not so good , hence the user 
	experience is not so good as compared to mlflow. also in MlFlow there are several integrated tools such as databricks 
	integration etc. Dvc is not so much mature as compared to the Mlflow.

mlflow solo mode -> only you can access your experiments , services ,tracking etc you get a local url 
mlflow collaboration mode -> you can share with your team etc.

** mlflow ui -> UI for tracking experiments.

* Diff between experiment and runs
experiment -> tells how you use differnt type of approaches to solve a problem , like for a problem such as student performance
		   prediction you take two approaches for predictions. one is that you use a random forest and another is 
		   feature embeddings based Neural network , so these are two separate experiments which you are trying out .

runs -> runs comes under each of these experiments where you try out different hyperparameters like in random forest , 
		we experiment with different n_estimators , max_depth etc.

* concepts of mlflow ui,
-> default is where your experiments get logged when you dont have set up your project or havent given any experiment name.

** with mlflow.start_run() -> context manager tool because of 'with' block othewise you have to write end_run() to terminate 
							current run.
							Start a new MLflow run, setting it as the active run under which metrics 
							and parameters will be logged.

** mlflow.set_experiment('iris-dt') --> set the experiment name under which it runs should come
** with mlflow.start_run(run_name) -> run_name -> you can give a name for your experiment.you can also give 
										description , run_id , experiment_id these two are pasted from ui mainly.

** mlflow.log_artifact('confusionmatrix.png') -> log the image or plots generally comes after 
												plt.savfig('confusionmatrix.png') cause it should save the artifact first

What can Mlflow track ??
-> parameters-> model hyperparameters , data parameters
-> metrics
-> Artifacts which mean tarcking data (not used much as dvc does this part majorly), source code , plot , environment info 
-> Models 
-> tags -> custom metadata
-> Run and experiment info

